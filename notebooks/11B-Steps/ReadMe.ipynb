{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### About the DataSet\n",
    "\n",
    "The full dataset includes:\n",
    "\n",
    " - 89,997,827 Customer documents\n",
    " - 89,997,827 IDcard documents\n",
    " - 179,998,862 Account documents\n",
    " - 179,998,862 Correspondance<State> documents\n",
    " - 179,998,862*60 Statement documents\n",
    "\n",
    "Total = 89,997,827*2+ 179,998,862*62 = 11,339,925,098\n",
    "\n",
    "The Data will be split in 3 repositories:\n",
    "\n",
    " - us-east\n",
    " \t- 50% of Custoners/IDCards/Accounts...\n",
    " \t- 6 months of statements\n",
    " - us-west\n",
    " \t- 50% of Custoners/IDCards/Accounts...\n",
    " \t- 6 months of statements\n",
    " - archives\n",
    " \t- 54 months of statements for all custoners\n",
    "\n",
    "This means:\n",
    "\n",
    " - us-east = 89,997,827 + 179,998,862 + 179,998,862/2*6 = 809,993,275\n",
    " - us-west = 89,997,827 + 179,998,862 + 179,998,862/2*6 = 809,993,275\n",
    " - archives = 179,998,862*54 = 9,719,938,548\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0 : run DocumentProducers \n",
    "\n",
    "The corresponding notebook can be executed [here](./Step%200%20-%20DocumentProducers.ipynb)\n",
    "\n",
    "#### Why making this a dedicated step?\n",
    "\n",
    "There are several reasons to make DocumentProducer a dedicated step\n",
    "\n",
    " - generating 9+B documents randomly takes time (see below)\n",
    " - it can use a lot of CPU at the injector level and I would like to avoid having to run more than one injector\n",
    " - once we have all DocumentMessages in Kafka, we can snapshot this and run DocumentConsumers separately\n",
    "\n",
    "#### Generation time\n",
    "\n",
    "Based on the previous tests the Producers throughput is:\n",
    "\n",
    " - 80K docs/s for CSV based import\n",
    " - 40K docs/s for Random Generation \n",
    "\n",
    "So, the projected producer time is: \n",
    "\n",
    " - Customers / Accounts hierarchy\n",
    " \t- 2*(89,997,827 + 179,998,862)/80000 = 6749 s => 2h\n",
    " - Non Archived statements \n",
    " \t- (179,998,862*6)/40000 = 26999s => 8h\n",
    " - Archived statements\n",
    " \t- (179,998,862*54)/40000 = 242998s => 68h\n",
    "\n",
    "#### Generation steps\n",
    "\n",
    " - import/states-hierarchy\n",
    " - import/customers\n",
    " - import/accounts\n",
    " - import/statements_live\n",
    " - import/statements_archive0\n",
    " - import/statements_archive1\n",
    " - import/statements_archive2\n",
    " - import/statements_archive3\n",
    " - import/statements_archive4\n",
    " - import/statements_archive5\n",
    " \n",
    "#### Volume Steps\n",
    "\n",
    " - V1 : 1.6B --> Tests\n",
    "   - import/states-hierarchy\n",
    "   - import/customers\n",
    "   - import/accounts\n",
    "   - import/statements_live\n",
    " - V2 : 2.6B --> Tests\n",
    "   - import/statements_archive0\n",
    " - V3 : 3.6B --> Tests\n",
    "   - import/statements_archive1\n",
    " - V3 : 4.6B --> Tests\n",
    "   - import/statements_archive2\n",
    " - V4 : 6.6B --> Tests\n",
    "   - import/statements_archive3\n",
    " - V5 : 8.6B --> Tests\n",
    "   - import/statements_archive4\n",
    " - V6 : 10.6B --> Tests\n",
    "   - import/statements_archive5\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 : Full Content but no Archives\n",
    "\n",
    "#### Target volumes\n",
    "\n",
    " - us-east = 809,993,275\n",
    " - us-west = 809,993,275\n",
    " - archives = 0\n",
    "\n",
    "#### Import steps\n",
    "\n",
    "##### run DocumentConsumers\n",
    "\n",
    "The idea is to run the consumers in bulk-mode and ideally in parallel against the 2 live repositories.\n",
    "\n",
    " - states\n",
    "    - import/states-hierarchy-us-east => us-east\n",
    "    - import/states-hierarchy-us-west => us-west\n",
    " - customers\n",
    "    - import/customers-us-east => us-east\n",
    "    - import/customers-us-west => us-west\n",
    " - accounts\n",
    "    - import/accounts-us-east => us-east\n",
    "    - import/accounts-us-west => us-west\n",
    " - statements\n",
    "    - import/statements_live-us-east => us-east\n",
    "    - import/statements_live-us-west => us-east\n",
    "\n",
    "\n",
    "| Node type  | sizing | count |\n",
    "|------|------|-----|\n",
    "| injector  | c5.2xlarge | 1 |\n",
    "| app  | m5.xlarge | 1 |\n",
    "| worker  | m5.xlarge | 1 |\n",
    "| mongodb  | M60 NVMe | 2 |\n",
    "| es-master  | r5.large.es | 3 |\n",
    "| es-data  | r5.2xlarge.es | 3 |\n",
    "\n",
    "\n",
    "\n",
    "##### run Bulk Indexing    \n",
    "\n",
    "The idea is to :\n",
    "\n",
    " - import all in us-east\n",
    " - start BAF Bulk Indexing on us-east\n",
    " - import all in us-west \n",
    " - start BAF Bulk Indexing on us-west\n",
    " \n",
    "In order to get Indexing as fast as possible:\n",
    "\n",
    " - scale the number of worker nodes: up to 8 ?\n",
    " - at this point the ES cluster should have 12 data nodes `r5.2xlarge.es`\n",
    " \n",
    "\n",
    "| Node type  | sizing | count |\n",
    "|------|------|-----|\n",
    "| injector  | c5.2xlarge | 1 |\n",
    "| app  | m5.xlarge | 1 |\n",
    "| worker  | m5.xlarge | 8 |\n",
    "| mongodb  | M60 NVMe | 2 |\n",
    "| es-master  | r5.large.es | 3 |\n",
    "| es-data  | r5.2xlarge.es | 12 |\n",
    "\n",
    "#### Testing\n",
    " \n",
    "\n",
    "\n",
    "| Node type  | sizing | count |\n",
    "|------|------|-----|\n",
    "| injector  | c5.2xlarge | 1 |\n",
    "| app  | m5.xlarge | 3 |\n",
    "| worker  | m5.xlarge | 2 |\n",
    "| mongodb  | M60 NVMe | 2 |\n",
    "| es-master  | r5.large.es | 3 |\n",
    "| es-data  | r5.2xlarge.es | 12 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 : 2.7B\n",
    "\n",
    "#### Target volumes\n",
    "\n",
    " - us-east = 809,993,275\n",
    " - us-west = 809,993,275\n",
    " - archives = 1,079,993,172\n",
    "\n",
    "#### Import steps\n",
    "\n",
    "##### run DocumentConsumers\n",
    "\n",
    "The idea is to run the consumers in bulk-mode and ideally in parallel against the 2 live repositories.\n",
    "\n",
    " - statements\n",
    "    - import/statements_archives0 => achives\n",
    "\n",
    "\n",
    "| Node type  | sizing | count |\n",
    "|------|------|-----|\n",
    "| injector  | c5.2xlarge | 1 |\n",
    "| app  | m5.xlarge | 1 |\n",
    "| worker  | m5.xlarge | 1 |\n",
    "| mongodb  | M60 NVMe | 2 |\n",
    "| mongodb  | M80 NVMe | 5 |\n",
    "| es-master  | r5.large.es | 3 |\n",
    "| es-data  | r5.2xlarge.es | 12 |\n",
    "\n",
    "\n",
    "##### run Bulk Indexing    \n",
    "\n",
    "In order to get Indexing as fast as possible:\n",
    "\n",
    " - scale the number of worker nodes: up to 8 ?\n",
    " - at this point the ES cluster should have 16 data nodes `r5.2xlarge.es`\n",
    " \n",
    "| Node type  | sizing | count |\n",
    "|------|------|-----|\n",
    "| injector  | c5.2xlarge | 1 |\n",
    "| app  | m5.xlarge | 1 |\n",
    "| worker  | m5.xlarge | 8 |\n",
    "| mongodb  | M60 NVMe | 2 |\n",
    "| mongodb  | M80 NVMe | 5 |\n",
    "| es-master  | r5.large.es | 3 |\n",
    "| es-data  | r5.2xlarge.es | 16 |\n",
    "\n",
    "#### Testing\n",
    " \n",
    "| Node type  | sizing | count |\n",
    "|------|------|-----|\n",
    "| injector  | c5.2xlarge | 1 |\n",
    "| app  | m5.xlarge | 3 |\n",
    "| worker  | m5.xlarge | 2 |\n",
    "| mongodb  | M60 NVMe | 2 |\n",
    "| mongodb  | M80 NVMe | 5 |\n",
    "| es-master  | r5.large.es | 3 |\n",
    "| es-data  | r5.2xlarge.es | 16 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other steps\n",
    "\n",
    "The other steps are likely to be very similar to the previous one, however:\n",
    "\n",
    " - we may need to adjust the number of ES nodes\n",
    " - we will try the new BAF feature allowing to directly pipe Importer and Bulk Indexing\n",
    " - we may do some adjustemnts on the tests\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
